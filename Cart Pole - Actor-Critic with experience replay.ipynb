{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a solution for the CartPole problem using Actor-Critic policy gradient method with experience replay where the critic predicts the state value function, the updates are based on TD(0), and the advantage function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$A^{\\pi_\\theta}(s,a) = r + \\gamma V_v(S_{t+1}) - V_v(S_t)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "display(Math(r'A^{\\pi_\\theta}(s,a) = r + \\gamma V_v(S_{t+1}) - V_v(S_t)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-05 20:32:59,317] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 49, Mean reward: 15\n",
      "Episode: 99, Mean reward: 14\n",
      "Episode: 149, Mean reward: 15\n",
      "Episode: 199, Mean reward: 20\n",
      "Episode: 249, Mean reward: 30\n",
      "Episode: 299, Mean reward: 52\n",
      "Episode: 349, Mean reward: 84\n",
      "Episode: 399, Mean reward: 144\n",
      "Episode: 449, Mean reward: 192\n",
      "Episode: 499, Mean reward: 184\n",
      "Episode: 549, Mean reward: 186\n",
      "Mean reward:  98.1541155867\n",
      "Max 100 rewards mean:  199.69\n",
      "Max 100 rewards from episode: 471, to episode: 570\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x43c705f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play 20 times with optimal policy\n",
      "Iteration: 0, Total Reward: 200\n",
      "Iteration: 1, Total Reward: 200\n",
      "Iteration: 2, Total Reward: 200\n",
      "Iteration: 3, Total Reward: 200\n",
      "Iteration: 4, Total Reward: 200\n",
      "Iteration: 5, Total Reward: 200\n",
      "Iteration: 6, Total Reward: 200\n",
      "Iteration: 7, Total Reward: 200\n",
      "Iteration: 8, Total Reward: 200\n",
      "Iteration: 9, Total Reward: 200\n",
      "Iteration: 10, Total Reward: 200\n",
      "Iteration: 11, Total Reward: 200\n",
      "Iteration: 12, Total Reward: 200\n",
      "Iteration: 13, Total Reward: 200\n",
      "Iteration: 14, Total Reward: 200\n",
      "Iteration: 15, Total Reward: 200\n",
      "Iteration: 16, Total Reward: 200\n",
      "Iteration: 17, Total Reward: 200\n",
      "Iteration: 18, Total Reward: 200\n",
      "Iteration: 19, Total Reward: 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "max_num_episodes = 10000\n",
    "checkpoint = 50\n",
    "experience_replay_maxlen = 10000\n",
    "batch_size = 128\n",
    "\n",
    "actor_learning_rate = 5e-3\n",
    "critic_learning_rate = 5e-4\n",
    "\n",
    "decay = 0.99\n",
    " \n",
    "class Actor:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self._X = tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
    "            self._rewards = tf.placeholder(dtype=tf.float32, shape=(None,1))\n",
    "            self._logits = tf.contrib.layers.fully_connected(self._X, 2, activation_fn=None)\n",
    "            self._policy = tf.contrib.layers.softmax(self._logits)\n",
    "            self._actions = tf.placeholder(dtype=tf.float32, shape=(None,2))\n",
    "            self._delta = tf.log(tf.reduce_sum(tf.mul(self._policy, self._actions), axis=1)) * self._rewards\n",
    "            self._train_step = tf.train.AdamOptimizer(learning_rate=actor_learning_rate).minimize(-1 * self._delta)\n",
    "\n",
    "    def predict_policy(self, state):\n",
    "        return self._policy.eval(feed_dict={self._X: state.reshape((-1, 4))})\n",
    "    \n",
    "    def train_policy(self, states, actions, rewards):\n",
    "        actions_mask = np.zeros((len(actions), 2))\n",
    "        actions_mask[range(len(actions)), actions] = 1\n",
    "        return self._train_step.run(feed_dict={self._X: states.reshape((-1, 4)), self._actions: actions_mask, self._rewards: rewards.reshape((-1,1))})\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('Critic'):\n",
    "            self._X = tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
    "            self._y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "            self._net = tf.contrib.layers.fully_connected(self._X, 30)\n",
    "            self._net = tf.contrib.layers.fully_connected(self._X, 30)\n",
    "            self._Q = tf.contrib.layers.fully_connected(self._net, 1, activation_fn=None)\n",
    "            self._mse = tf.contrib.losses.mean_squared_error(self._y, self._Q)\n",
    "            self._train_step = tf.train.AdamOptimizer(learning_rate=critic_learning_rate).minimize(self._mse)\n",
    "\n",
    "    def predict_Q(self, state):\n",
    "        return self._Q.eval(feed_dict={self._X: state.reshape((-1, 4))})\n",
    "    \n",
    "    def train_Q(self, states, targets):\n",
    "        return self._train_step.run(feed_dict={self._X: states.reshape((-1, 4)), self._y: targets.reshape((-1, 1))})\n",
    "        \n",
    "def stats(rewards):\n",
    "    print(\"Mean reward: \", np.mean(rewards))\n",
    "    rewards_100 = []\n",
    "    for i in range(100, len(rewards) + 1):\n",
    "        rewards_100.append(np.mean(rewards[i-100:i]))\n",
    "    print(\"Max 100 rewards mean: \", np.max(rewards_100))\n",
    "    re = np.argmax(rewards_100)\n",
    "    print(\"Max 100 rewards from episode: %d, to episode: %d\" % (re, re + 99))\n",
    "    plt.plot(rewards_100)\n",
    "    plt.show()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "all_rewards = []\n",
    "\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "experiences_memory = deque(maxlen=experience_replay_maxlen)\n",
    "for episode in range(max_num_episodes):\n",
    "    state = env.reset()\n",
    "    epsilon = 1./((episode/50) + 10)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = np.random.choice(range(2), p=actor.predict_policy(state)[0])\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Append final reward for each episode\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save experience\n",
    "        experiences_memory.append([state, action, reward, next_state, done])\n",
    "\n",
    "        # Switch to next state\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        if len(experiences_memory) >= batch_size:\n",
    "            experiences = random.sample(experiences_memory, batch_size)\n",
    "            \n",
    "            states = np.vstack([x[0] for x in experiences])\n",
    "            actions = np.array([x[1] for x in experiences])\n",
    "            rewards = np.array([x[2] for x in experiences])\n",
    "            next_states = np.vstack([x[3] for x in experiences])\n",
    "            episode_done = np.array([x[4] for x in experiences])\n",
    "\n",
    "            curr_v = critic.predict_Q(states)\n",
    "            next_v = critic.predict_Q(next_states)\n",
    "            bootstrapped_value = rewards + decay * next_v[:, 0] * ~episode_done\n",
    "            actor.train_policy(states, actions, bootstrapped_value - curr_v[:, 0])\n",
    "            critic.train_Q(states, bootstrapped_value)\n",
    "            \n",
    "\n",
    "    if (episode + 1) % checkpoint == 0:\n",
    "        print(\"Episode: %d, Mean reward: %d\" % (episode, np.mean(all_rewards[-100:])))\n",
    "    \n",
    "    all_rewards.append(episode_reward)\n",
    "    \n",
    "    if np.mean(all_rewards[-100:]) > 199.5:\n",
    "        break\n",
    "\n",
    "stats(all_rewards)\n",
    "\n",
    "print(\"Play 20 times with optimal policy\")\n",
    "for i in range(20):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state, reward, done, _ = env.step(np.random.choice(range(2), p=actor.predict_policy(state)[0]))\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "    print(\"Iteration: %d, Total Reward: %d\" % (i, total_reward))\n",
    "    \n",
    "sess.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
